{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b8ec68-2a86-4e9f-9609-69330e2f659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Original Raw Data:\n",
      "+---+-----------------+\n",
      "|id |raw_name         |\n",
      "+---+-----------------+\n",
      "|1  |Alice   Smith!!  |\n",
      "|2  |bob-smith        |\n",
      "|3  |CHARLIE   Johnson|\n",
      "|4  |david johnson    |\n",
      "|5  |Alice SMITH      |\n",
      "+---+-----------------+\n",
      "\n",
      "✅ Normalized Data:\n",
      "+---+-----------------+-----------------+\n",
      "|id |raw_name         |clean_name       |\n",
      "+---+-----------------+-----------------+\n",
      "|1  |Alice   Smith!!  |alice   smith    |\n",
      "|2  |bob-smith        |bobsmith         |\n",
      "|3  |CHARLIE   Johnson|charlie   johnson|\n",
      "|4  |david johnson    |david johnson    |\n",
      "|5  |Alice SMITH      |alice smith      |\n",
      "+---+-----------------+-----------------+\n",
      "\n",
      "✅ Tokenized Data:\n",
      "+---+-----------------+-----------------+------------------+\n",
      "|id |raw_name         |clean_name       |tokens            |\n",
      "+---+-----------------+-----------------+------------------+\n",
      "|1  |Alice   Smith!!  |alice   smith    |[alice, smith]    |\n",
      "|2  |bob-smith        |bobsmith         |[bobsmith]        |\n",
      "|3  |CHARLIE   Johnson|charlie   johnson|[charlie, johnson]|\n",
      "|4  |david johnson    |david johnson    |[david, johnson]  |\n",
      "|5  |Alice SMITH      |alice smith      |[alice, smith]    |\n",
      "+---+-----------------+-----------------+------------------+\n",
      "\n",
      "✅ Exploded Tokens (one word per row):\n",
      "+---+-----------------+-----------------+------------------+--------+\n",
      "|id |raw_name         |clean_name       |tokens            |token   |\n",
      "+---+-----------------+-----------------+------------------+--------+\n",
      "|1  |Alice   Smith!!  |alice   smith    |[alice, smith]    |alice   |\n",
      "|1  |Alice   Smith!!  |alice   smith    |[alice, smith]    |smith   |\n",
      "|2  |bob-smith        |bobsmith         |[bobsmith]        |bobsmith|\n",
      "|3  |CHARLIE   Johnson|charlie   johnson|[charlie, johnson]|charlie |\n",
      "|3  |CHARLIE   Johnson|charlie   johnson|[charlie, johnson]|johnson |\n",
      "|4  |david johnson    |david johnson    |[david, johnson]  |david   |\n",
      "|4  |david johnson    |david johnson    |[david, johnson]  |johnson |\n",
      "|5  |Alice SMITH      |alice smith      |[alice, smith]    |alice   |\n",
      "|5  |Alice SMITH      |alice smith      |[alice, smith]    |smith   |\n",
      "+---+-----------------+-----------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, trim, explode\n",
    "\n",
    "# --- Initialize Spark ---\n",
    "spark = SparkSession.builder.appName(\"Task6_DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# --- Sample Raw Data (simulating dirty entity names) ---\n",
    "data = [\n",
    "    (1, \"Alice   Smith!!\"),\n",
    "    (2, \"bob-smith\"),\n",
    "    (3, \"CHARLIE   Johnson\"),\n",
    "    (4, \"david johnson\"),\n",
    "    (5, \"Alice SMITH\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"raw_name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"✅ Original Raw Data:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# --- Step 1: Normalization ---\n",
    "# - Convert to lowercase\n",
    "# - Remove punctuation/special characters\n",
    "# - Trim spaces\n",
    "df_clean = df.withColumn(\"clean_name\",\n",
    "    lower(trim(regexp_replace(col(\"raw_name\"), \"[^a-zA-Z\\\\s]\", \"\")))\n",
    ")\n",
    "\n",
    "print(\"✅ Normalized Data:\")\n",
    "df_clean.show(truncate=False)\n",
    "\n",
    "# --- Step 2: Tokenization ---\n",
    "# Split names into tokens (words)\n",
    "df_tokens = df_clean.withColumn(\"tokens\", split(col(\"clean_name\"), \"\\\\s+\"))\n",
    "\n",
    "print(\"✅ Tokenized Data:\")\n",
    "df_tokens.show(truncate=False)\n",
    "\n",
    "# --- Step 3: Explode tokens (optional for entity resolution) ---\n",
    "df_exploded = df_tokens.withColumn(\"token\", explode(col(\"tokens\")))\n",
    "\n",
    "print(\"✅ Exploded Tokens (one word per row):\")\n",
    "df_exploded.show(truncate=False)\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174d27a-a379-4db3-9199-d58b70ff88b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigdataenv)",
   "language": "python",
   "name": "bigdataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
